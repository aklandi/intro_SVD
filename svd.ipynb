{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "## A Python Lab for Undergraduate Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy to deal with matrices\n",
    "import numpy as np\n",
    "# import graphing library\n",
    "import matplotlib.pyplot as plt\n",
    "# import library that deals with image files\n",
    "from PIL import Image\n",
    "# import library that loads built-in datasets\n",
    "from sklearn import datasets, model_selection\n",
    "# import library to analyze data sets\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application: Data Compression & De-Noising**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "img = Image.open(\"D:\\phyll.png\")\n",
    "# turn color image to gray: two-dimensional matrix v. three-dimensional\n",
    "gray = img.convert('LA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We like to convert color images into gray scale because the gray scale is encoded by the integers 0:255, AND each element in the image corresponds to an element in an M-by-N matrix.  Color images are M-by-N-by-3 matrices because each pixel has a red-green-blue encoding;  It's easier not to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display image so you can see it\n",
    "plt.imshow(gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a Dwarven Bard whose primary instruments are the maracas and the kazoo using HeroForge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the image object to a numpy array to use the SVD function\n",
    "A = np.array(list(gray.getdata(band = 0)), float)\n",
    "A.shape = (gray.size[1], gray.size[0])\n",
    "A = np.matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display matrix so you can see\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use numpy's svd function in its linear algebra sub-library.  There are other svd functions (a curse and a blessing of open source languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll check out the magnitudes of the singular values.  Recall they're ordered based on largest, $\\sigma_1$, to smallest, $\\sigma_r$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the huge drop off in value!  Somewhere between $\\sigma_5$ and $\\sigma_{20}$.  In the word of data compression, this says to us that the other components don't matter because their singular values are so tiny relative the the first handful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,50):\n",
    "    p = i;\n",
    "    approx = np.matrix(U[:,:p])*np.diag(S[:p])*np.matrix(V[:p,:]);\n",
    "    plt.imshow(approx, cmap = 'gray')\n",
    "    print(\"Number is \", p)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we'll extract the singular vectors and singular values associated only with those first 20 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 20\n",
    "approx = np.matrix(U[:,:p])*np.diag(S[:p])*np.matrix(V[:p,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(approx, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get a relatively decent idea of what the image entails.  Not perfect, but it doesn't have to be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check mathematically how accurate of a prediction the approximate image is to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.linalg.norm(approx - A)/np.linalg.norm(A)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this process over, but we'll do it with noise added to the original image.  The idea is that the singular values will highlight directions that highlight important features of the image, sifting through the noise.  The largest singular values should be able to indicate to use which components have minimal noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_A = np.matrix(A) + 50*np.random.normal(0, 1, A.shape)\n",
    "plt.imshow(noise_A, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = np.linalg.svd(noise_A)\n",
    "plt.plot(S)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 20\n",
    "approx_noise = np.matrix(U[:,:p])*np.diag(S[:p])*np.matrix(V[:p,:])\n",
    "plt.imshow(approx_noise, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.linalg.norm(approx_noise - A)/np.linalg.norm(A)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little worse than before, but I think we were able to figure out the best representation despite the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application: Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression, we want to be able to take at least one independent variable $\\{{\\bf x_1}, {\\bf x_2}, \\dots, {\\bf x_n} \\}$ and make inferences about its relationship to some dependent variable, ${\\bf y}$.  In linear algebra, we attempt to solve the problem\n",
    "$$\n",
    "\\min_{{\\bf a}}|| {\\bf y} - {\\bf Xa}||_F,\n",
    "$$\n",
    "where ${\\bf a}$ will contain the information for the slopes for each ${\\bf x_i}$ and for the intercept value.  \n",
    "\n",
    "We'll use SVD to help give us a decomposition for ${\\bf X}$ and then\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "{\\bf y} = {\\bf X a}\\\\\n",
    "{\\bf y} = {\\bf U\\Sigma V^{T} a}\\\\\n",
    "{\\bf V \\Sigma^{-1}U^{T}y} = {\\bf a}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load a housing dataset for houses in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import built-in data set from Python\n",
    "DF = datasets.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code only helps to display the data neatly, don't need it for later.\n",
    "X = pd.DataFrame(DF['data'], columns = DF['feature_names']); #X = X.style.hide(axis='index')\n",
    "X.rename({0:\"rowindx\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code only helps to display the data neatly, don't need it for later.\n",
    "Y = pd.DataFrame(DF['target'], columns=DF['target_names']);#Y = Y.style.hide(axis='index')\n",
    "Y.rename({0:\"rowindx\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now extract the *observed* data ${\\bf X}$ and ${\\bf y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DF['data']\n",
    "y = DF['target']\n",
    "X = np.column_stack([np.ones(X.shape[0]), X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data into a \"training\" and a \"test\" set of ${\\bf X}$ and a \"training\" and \"test\" set of ${\\bf y}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the SVD of the training ${\\bf X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = np.linalg.svd(X_train, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what these singular values look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUCH more dramatic than before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use SVD to help give us a decomposition for training ${\\bf X}$ and then\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "{\\bf y} = {\\bf X a}\\\\\n",
    "{\\bf y} = {\\bf U\\Sigma V^{T} a}\\\\\n",
    "{\\bf V \\Sigma^{-1}U^{T}y} = {\\bf a}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = V.T @ np.linalg.inv(np.diag(S)) @ U.T @ Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this ${\\bf a}$ is based only on the training data, then it is only an estimate of what the relationship between ${\\bf X}$ and ${\\bf y}$ should be.  We'll see how accurate this estimated relationship is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.matrix(X_train)*np.matrix(a)\n",
    "test_predictions = np.matrix(X_test)*np.matrix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(Y_train - Y_pred)/np.linalg.norm(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's okay..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_predictions - Y_test)/np.linalg.norm(Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
